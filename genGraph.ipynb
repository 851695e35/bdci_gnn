{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liqiang/software/miniconda3/envs/pyg/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.data import Data\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取dataframe并且处理一下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取node\n",
    "node_df_train = pd.read_csv(\"data/raw/train_90.csv\")\n",
    "# 删除全0列\n",
    "drop_c = [\"F_23\", \"F_27\"]\n",
    "node_df_train.drop(drop_c, axis=1, inplace=True)\n",
    "node_ids = node_df_train[\"geohash_id\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_ids.index(\"5324516fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 标准化\n",
    "id_and_date_columns = [\"geohash_id\", \"date_id\"]\n",
    "label_columns = [\"active_index\", \"consume_index\"]\n",
    "feature_columns = node_df_train.drop(\n",
    "    id_and_date_columns + label_columns, axis=1\n",
    ").columns\n",
    "\n",
    "# feature_scaler = StandardScaler()\n",
    "# node_df_train.loc[:, feature_columns] = feature_scaler.fit_transform(\n",
    "#     node_df_train[feature_columns]\n",
    "# )\n",
    "# label_scaler = StandardScaler()\n",
    "# node_df_train.loc[:, label_columns] = label_scaler.fit_transform(\n",
    "#     node_df_train[label_columns]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读边"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_df_train = pd.read_csv(\"data/raw/edge_90.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化\n",
    "edge_feature_columns = [\"F_1\", \"F_2\"]\n",
    "edge_feature_scaler = StandardScaler()\n",
    "edge_df_train.loc[:, edge_feature_columns] = edge_feature_scaler.fit_transform(\n",
    "    edge_df_train[edge_feature_columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildGraph(node_df, edge_df, is_test_dataset=False):\n",
    "    graphs = []\n",
    "\n",
    "    # 按照date_id分组\n",
    "    for date_id, graph in node_df.groupby(\"date_id\"):\n",
    "        # 节点\n",
    "        x = torch.tensor(graph[feature_columns].values, dtype=torch.float)\n",
    "        if not is_test_dataset:\n",
    "            y = torch.tensor(graph[label_columns].values, dtype=torch.float)\n",
    "\n",
    "        # 边\n",
    "        edge_day_df = edge_df[edge_df[\"date_id\"] == date_id]\n",
    "        edge_index, edge_attr = [], []\n",
    "        for _, edge in edge_day_df.iterrows():\n",
    "            # 边可能给多了，只取存在的\n",
    "            if (\n",
    "                edge[\"geohash6_point1\"] not in node_ids\n",
    "                or edge[\"geohash6_point2\"] not in node_ids\n",
    "            ):\n",
    "                continue\n",
    "            edge_index.append(\n",
    "                [\n",
    "                    node_ids.index(edge[\"geohash6_point1\"]),\n",
    "                    node_ids.index(edge[\"geohash6_point2\"]),\n",
    "                ]\n",
    "            )\n",
    "            edge_attr.append(edge[edge_feature_columns].values.astype(np.float32))\n",
    "        edge_index = (\n",
    "            torch.tensor(np.array(edge_index), dtype=torch.long).t().contiguous()\n",
    "        )\n",
    "        edge_attr = torch.tensor(np.array(edge_attr), dtype=torch.float)\n",
    "\n",
    "        # 图\n",
    "        if is_test_dataset:\n",
    "            graph = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        else:\n",
    "            graph = Data(x=x, y=y, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        graphs.append(graph)\n",
    "\n",
    "        print(date_id, \"finished\")\n",
    "\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20230104 finished\n",
      "20230105 finished\n",
      "20230106 finished\n",
      "20230107 finished\n",
      "20230108 finished\n",
      "20230109 finished\n",
      "20230110 finished\n",
      "20230111 finished\n",
      "20230112 finished\n",
      "20230113 finished\n",
      "20230114 finished\n",
      "20230115 finished\n",
      "20230116 finished\n",
      "20230117 finished\n",
      "20230118 finished\n",
      "20230119 finished\n",
      "20230120 finished\n",
      "20230121 finished\n",
      "20230122 finished\n",
      "20230123 finished\n",
      "20230124 finished\n",
      "20230125 finished\n",
      "20230126 finished\n",
      "20230127 finished\n",
      "20230128 finished\n",
      "20230129 finished\n",
      "20230130 finished\n",
      "20230131 finished\n",
      "20230201 finished\n",
      "20230202 finished\n",
      "20230203 finished\n",
      "20230204 finished\n",
      "20230205 finished\n",
      "20230206 finished\n",
      "20230207 finished\n",
      "20230208 finished\n",
      "20230209 finished\n",
      "20230210 finished\n",
      "20230211 finished\n",
      "20230212 finished\n",
      "20230213 finished\n",
      "20230214 finished\n",
      "20230215 finished\n",
      "20230216 finished\n",
      "20230217 finished\n",
      "20230218 finished\n",
      "20230219 finished\n",
      "20230220 finished\n",
      "20230221 finished\n",
      "20230222 finished\n",
      "20230223 finished\n",
      "20230224 finished\n",
      "20230225 finished\n",
      "20230226 finished\n",
      "20230227 finished\n",
      "20230228 finished\n",
      "20230301 finished\n",
      "20230302 finished\n",
      "20230303 finished\n",
      "20230304 finished\n",
      "20230305 finished\n",
      "20230306 finished\n",
      "20230307 finished\n",
      "20230308 finished\n",
      "20230309 finished\n",
      "20230310 finished\n",
      "20230311 finished\n",
      "20230312 finished\n",
      "20230313 finished\n",
      "20230314 finished\n",
      "20230315 finished\n",
      "20230316 finished\n",
      "20230317 finished\n",
      "20230318 finished\n",
      "20230319 finished\n",
      "20230320 finished\n",
      "20230321 finished\n",
      "20230322 finished\n",
      "20230323 finished\n",
      "20230324 finished\n",
      "20230325 finished\n",
      "20230326 finished\n",
      "20230327 finished\n",
      "20230328 finished\n",
      "20230329 finished\n",
      "20230330 finished\n",
      "20230331 finished\n",
      "20230401 finished\n",
      "20230402 finished\n",
      "20230403 finished\n"
     ]
    }
   ],
   "source": [
    "graphs = buildGraph(node_df_train, edge_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveData(data, path):\n",
    "    with open(path, \"wb\") as file:\n",
    "        pickle.dump(data, file)\n",
    "        file.close()\n",
    "\n",
    "\n",
    "saveData(graphs, \"data/train_graphs_nostd_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存node_id和index的对应关系\n",
    "saveData(node_ids, \"data/node_ids.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成训练集图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取node\n",
    "node_df_test = pd.read_csv(\"data/raw/A榜/node_test_4_A.csv\")\n",
    "node_df_test.drop(drop_c, axis=1, inplace=True)\n",
    "with open(\"data/node_ids.pkl\", \"rb\") as file:\n",
    "    # 和训练集的node_ids保持一致\n",
    "    node_ids = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试集中有一些不正确的node_id，需要进行转化\n",
    "special_node = {\n",
    "    \"18377236\": \"018377236\",\n",
    "    \"7.45E+07\": \"7449766e1\",\n",
    "    \"9.80E+10\": \"9797336e4\",\n",
    "}\n",
    "node_df_test.replace(special_node, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在测试集中的node_id都是正确的\n",
    "assert node_df_test[\"geohash_id\"].isin(node_ids).value_counts().item() == len(\n",
    "    node_df_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 标准化\n",
    "# node_df_test.loc[:, feature_columns] = feature_scaler.transform(\n",
    "#     node_df_test[feature_columns]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "边"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_df_test = pd.read_csv(\"data/raw/A榜/edge_test_4_A.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化\n",
    "edge_df_test.loc[:, edge_feature_columns] = edge_feature_scaler.transform(\n",
    "    edge_df_test[edge_feature_columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20230404 finished\n",
      "20230405 finished\n",
      "20230406 finished\n",
      "20230407 finished\n"
     ]
    }
   ],
   "source": [
    "test_graphs = buildGraph(node_df_test, edge_df_test, is_test_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveData(test_graphs, \"data/test_graphs_nostd_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存所有标准化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveData(feature_scaler, \"saved/scaler/feature_scaler.pkl\")\n",
    "# saveData(label_scaler, \"saved/scaler/label_scaler.pkl\")\n",
    "saveData(edge_feature_scaler, \"saved/scaler/edge_feature_scaler.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
