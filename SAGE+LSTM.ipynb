{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "114502f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch_geometric.nn as gnn\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.functional import F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "with open(\"data/train_graphs_data.pkl\", \"rb\") as file:\n",
    "    all_graphs = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90,\n",
       " Data(x=[1140, 33], edge_index=[2, 11709], edge_attr=[11709, 2], y=[1140, 2]),\n",
       " tensor([-0.2741, -0.6940]))"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_graphs), all_graphs[0], all_graphs[0].y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 制作模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAGE用于对图进行embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b3d10089",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGEModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SAGEModel, self).__init__()\n",
    "\n",
    "        self.graph_features = nn.ModuleList(\n",
    "            [\n",
    "                gnn.SAGEConv(input_size, 128),\n",
    "                nn.LayerNorm(128),\n",
    "                nn.ReLU(),\n",
    "                gnn.SAGEConv(128, 128),\n",
    "                nn.LayerNorm(128),\n",
    "                nn.ReLU(),\n",
    "                gnn.SAGEConv(128, 128),\n",
    "                nn.LayerNorm(128),\n",
    "                nn.ReLU(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 考虑更多的trick，如layernorm等等\n",
    "        self.regression = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LayerNorm(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(32, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, _ = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        output = x\n",
    "\n",
    "        for layer in self.graph_features:\n",
    "            if isinstance(layer, gnn.SAGEConv):\n",
    "                output = layer(output, edge_index)\n",
    "            else:\n",
    "                output = layer(output)\n",
    "\n",
    "        output = self.regression(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1140, 32])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 看看模型的输入输出\n",
    "model = SAGEModel(input_size=33, output_size=32)\n",
    "output = model(all_graphs[0])\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM用于串联每个节点的embedding，并计算回归值\n",
    "\n",
    "输入：x.shape = (1140, seq_len, 节点feature_size)\n",
    "\n",
    "**注意：第一个维度只能是1140，而不能自定义batch_size，因为图模型跑完以后会得到整张图的embedding**\n",
    "\n",
    "如果选取一小部分放进lstm，SAGE+LSTM模型的forward函数将变得很难写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.LSTM(\n",
    "                input_size=input_size,\n",
    "                hidden_size=64,\n",
    "                num_layers=1,\n",
    "                batch_first=True,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.linears = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, (_, _) = self.features(x)\n",
    "        out = self.linears(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1140, 90, 33])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_input = np.array([g.x for g in all_graphs])\n",
    "# batch放到第一个维度\n",
    "lstm_input = lstm_input.swapaxes(0, 1)\n",
    "lstm_input = torch.from_numpy(lstm_input).float()\n",
    "\n",
    "lstm_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1140, 90, 2])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMModel(input_size=33, output_size=2)\n",
    "output = model(lstm_input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总的模型（下面简称为SL模型）：先图conv，再lstm\n",
    "\n",
    "输入：图列表，长度为LSTM的seq_len\n",
    "\n",
    "输出：[1140, seq_len, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGEandLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SAGEandLSTMModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.sage = SAGEModel(input_size, 32)\n",
    "        self.lstm = LSTMModel(32, output_size)\n",
    "\n",
    "    def forward(self, graphs):\n",
    "        # data: [graph]\n",
    "        # 堆叠在维度1\n",
    "        # output.shape = [1140个节点, 90张图, 32个特征]\n",
    "\n",
    "        # 这里能不能并行呢？能不能不用for循环？\n",
    "        output = torch.stack([self.sage(graph) for graph in graphs], dim=1)\n",
    "        output = self.lstm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1140, 90, 2])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SAGEandLSTMModel(input_size=33, output_size=2)\n",
    "output = model(all_graphs)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 制作数据\n",
    "\n",
    "数据需要处理成SL模型需要的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据全部放到GPU上\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "graphs = [graph.to(device) for graph in all_graphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genLSTMData(graphs, seq_len, stride=1):\n",
    "    # graphs：列表，每个元素是一个graph\n",
    "    graph_list = []\n",
    "    for i in range(0, len(graphs) - seq_len + 1, stride):\n",
    "        print(i, i + seq_len)\n",
    "        graph_list.append(graphs[i : i + seq_len])\n",
    "    return graph_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7\n",
      "2 9\n",
      "4 11\n",
      "6 13\n",
      "8 15\n",
      "10 17\n",
      "12 19\n",
      "14 21\n",
      "16 23\n",
      "18 25\n",
      "20 27\n",
      "22 29\n",
      "24 31\n",
      "26 33\n",
      "28 35\n",
      "30 37\n",
      "32 39\n",
      "34 41\n",
      "36 43\n",
      "38 45\n",
      "40 47\n",
      "42 49\n",
      "44 51\n",
      "46 53\n",
      "48 55\n",
      "50 57\n",
      "52 59\n",
      "54 61\n",
      "56 63\n",
      "58 65\n",
      "60 67\n",
      "62 69\n",
      "64 71\n",
      "66 73\n",
      "68 75\n",
      "70 77\n",
      "72 79\n",
      "74 81\n",
      "76 83\n",
      "78 85\n",
      "80 87\n",
      "82 89\n"
     ]
    }
   ],
   "source": [
    "graph_list = genLSTMData(graphs, 7, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 7, 7)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_list), len(graph_list[0]), len(graph_list[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分训练测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和验证集，使用sklearn的train_test_split函数\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_graph_list, val_graph_list = train_test_split(graph_list, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "bb002ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 2, 2, device(type='cuda'))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = graphs[0].x.shape[1]\n",
    "# 这里的output_size就是最终的输出维度，不需要再乘以num_heads\n",
    "output_size = graphs[0].y.shape[1]\n",
    "# 个人认为，注意力头的个数至少应该等于输出维度，因为每个输出可能需要关注不同的邻居\n",
    "num_heads = graphs[0].y.shape[1]\n",
    "\n",
    "# batch_size = 256\n",
    "input_size, output_size, num_heads, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c1d2aa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    train_graph_list,\n",
    "    val_graph_list=None,\n",
    "    num_epochs=50,\n",
    "):\n",
    "    # 训练过程记录\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "\n",
    "    with tqdm(total=num_epochs, desc=\"Training Progress\", unit=\"epoch\") as pbar_epochs:\n",
    "        for epoch in range(num_epochs):\n",
    "            # 训练\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "\n",
    "            # 图网络不支持batch，只能一个一个图训练\n",
    "            for i, graphs in enumerate(train_graph_list):\n",
    "                output = model(graphs)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(\n",
    "                    output, torch.stack([graph.y for graph in graphs], dim=1)\n",
    "                )\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            train_loss /= len(train_graph_list)\n",
    "            train_loss_list.append(train_loss)\n",
    "\n",
    "            # 验证\n",
    "            if val_graph_list:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_loss = 0.0\n",
    "                    for i, graphs in enumerate(val_graph_list):\n",
    "                        output = model(graphs)\n",
    "                        loss = criterion(\n",
    "                            output, torch.stack([graph.y for graph in graphs], dim=1)\n",
    "                        )\n",
    "                        val_loss += loss.item()\n",
    "                    val_loss /= len(val_graph_list)\n",
    "                    val_loss_list.append(val_loss)\n",
    "\n",
    "            if val_graph_list:\n",
    "                pbar_epochs.set_postfix(\n",
    "                    {\"train MSE Loss\": train_loss, \"val MSE Loss\": val_loss}\n",
    "                )\n",
    "            else:\n",
    "                pbar_epochs.set_postfix({\"train MSE Loss\": train_loss})\n",
    "\n",
    "\n",
    "            pbar_epochs.update(1)\n",
    "            # 学习率更新\n",
    "            scheduler.step()\n",
    "\n",
    "    # 可视化训练过程\n",
    "    plt.figure()\n",
    "    plt.plot(train_loss_list, label=\"train loss\")\n",
    "    if val_graph_list:\n",
    "        plt.plot(val_loss_list, label=\"val loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"MSE loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    return model, train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/300 [00:00<?, ?epoch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  55%|█████▌    | 165/300 [03:14<02:39,  1.18s/epoch, train MSE Loss=0.0093, val MSE Loss=0.00607] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE+LSTM.ipynb 单元格 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# criterion = nn.MSELoss(reduction=\"sum\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m model \u001b[39m=\u001b[39m train(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     criterion\u001b[39m=\u001b[39;49mcriterion,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     scheduler\u001b[39m=\u001b[39;49mscheduler,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     train_graph_list\u001b[39m=\u001b[39;49mtrain_graph_list,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     val_graph_list\u001b[39m=\u001b[39;49mval_graph_list,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     num_epochs\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m )\n",
      "\u001b[1;32m/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE+LSTM.ipynb 单元格 24\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# 图网络不支持batch，只能一个一个图训练\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, graphs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_graph_list):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     output \u001b[39m=\u001b[39m model(graphs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m         output, torch\u001b[39m.\u001b[39mstack([graph\u001b[39m.\u001b[39my \u001b[39mfor\u001b[39;00m graph \u001b[39min\u001b[39;00m graphs], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/software/miniconda3/envs/pyg/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE+LSTM.ipynb 单元格 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, graphs):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# data: [graph]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# 堆叠在维度1\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# output.shape = [1140个节点, 90张图, 32个特征]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# 这里能不能并行呢？能不能不用for循环？\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msage(graph) \u001b[39mfor\u001b[39;00m graph \u001b[39min\u001b[39;00m graphs], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm(output)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n",
      "\u001b[1;32m/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE+LSTM.ipynb 单元格 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, graphs):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# data: [graph]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# 堆叠在维度1\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# output.shape = [1140个节点, 90张图, 32个特征]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# 这里能不能并行呢？能不能不用for循环？\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msage(graph) \u001b[39mfor\u001b[39;00m graph \u001b[39min\u001b[39;00m graphs], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm(output)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/software/miniconda3/envs/pyg/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE+LSTM.ipynb 单元格 24\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph_features:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(layer, gnn\u001b[39m.\u001b[39mSAGEConv):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m         output \u001b[39m=\u001b[39m layer(output, edge_index)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.60.150.62/home/liqiang/workspace/datamining/datamining_dataset_1110/graph/bdci_gnn/SAGE%2BLSTM.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m         output \u001b[39m=\u001b[39m layer(output)\n",
      "File \u001b[0;32m~/software/miniconda3/envs/pyg/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/software/miniconda3/envs/pyg/lib/python3.10/site-packages/torch_geometric/nn/conv/sage_conv.py:130\u001b[0m, in \u001b[0;36mSAGEConv.forward\u001b[0;34m(self, x, edge_index, size)\u001b[0m\n\u001b[1;32m    127\u001b[0m     x \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin(x[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mrelu(), x[\u001b[39m1\u001b[39m])\n\u001b[1;32m    129\u001b[0m \u001b[39m# propagate_type: (x: OptPairTensor)\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate(edge_index, x\u001b[39m=\u001b[39;49mx, size\u001b[39m=\u001b[39;49msize)\n\u001b[1;32m    131\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin_l(out)\n\u001b[1;32m    133\u001b[0m x_r \u001b[39m=\u001b[39m x[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/software/miniconda3/envs/pyg/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:480\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m         aggr_kwargs \u001b[39m=\u001b[39m res[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(res, \u001b[39mtuple\u001b[39m) \u001b[39melse\u001b[39;00m res\n\u001b[0;32m--> 480\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maggregate(out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49maggr_kwargs)\n\u001b[1;32m    482\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_aggregate_forward_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m    483\u001b[0m     res \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, (aggr_kwargs, ), out)\n",
      "File \u001b[0;32m~/software/miniconda3/envs/pyg/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:604\u001b[0m, in \u001b[0;36mMessagePassing.aggregate\u001b[0;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maggregate\u001b[39m(\u001b[39mself\u001b[39m, inputs: Tensor, index: Tensor,\n\u001b[1;32m    592\u001b[0m               ptr: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    593\u001b[0m               dim_size: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    594\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Aggregates messages from neighbors as\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[39m    :math:`\\bigoplus_{j \\in \\mathcal{N}(i)}`.\u001b[39;00m\n\u001b[1;32m    596\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[39m    as specified in :meth:`__init__` by the :obj:`aggr` argument.\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 604\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maggr_module(inputs, index, ptr\u001b[39m=\u001b[39;49mptr, dim_size\u001b[39m=\u001b[39;49mdim_size,\n\u001b[1;32m    605\u001b[0m                             dim\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim)\n",
      "File \u001b[0;32m~/software/miniconda3/envs/pyg/lib/python3.10/site-packages/torch_geometric/experimental.py:115\u001b[0m, in \u001b[0;36mdisable_dynamic_shapes.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_experimental_mode_enabled(\u001b[39m'\u001b[39m\u001b[39mdisable_dynamic_shapes\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    117\u001b[0m     \u001b[39mfor\u001b[39;00m required_arg \u001b[39min\u001b[39;00m required_args:\n\u001b[1;32m    118\u001b[0m         index \u001b[39m=\u001b[39m required_args_pos[required_arg]\n",
      "File \u001b[0;32m~/software/miniconda3/envs/pyg/lib/python3.10/site-packages/torch_geometric/nn/aggr/base.py:125\u001b[0m, in \u001b[0;36mAggregation.__call__\u001b[0;34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m     dim_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(index\u001b[39m.\u001b[39mmax()) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m index\u001b[39m.\u001b[39mnumel() \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m    124\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(x, index\u001b[39m=\u001b[39;49mindex, ptr\u001b[39m=\u001b[39;49mptr, dim_size\u001b[39m=\u001b[39;49mdim_size,\n\u001b[1;32m    126\u001b[0m                             dim\u001b[39m=\u001b[39;49mdim, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    127\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mIndexError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/software/miniconda3/envs/pyg/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/software/miniconda3/envs/pyg/lib/python3.10/site-packages/torch_geometric/nn/aggr/basic.py:35\u001b[0m, in \u001b[0;36mMeanAggregation.forward\u001b[0;34m(self, x, index, ptr, dim_size, dim)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor, index: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     33\u001b[0m             ptr: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, dim_size: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     34\u001b[0m             dim: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 35\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduce(x, index, ptr, dim_size, dim, reduce\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/software/miniconda3/envs/pyg/lib/python3.10/site-packages/torch_geometric/nn/aggr/base.py:176\u001b[0m, in \u001b[0;36mAggregation.reduce\u001b[0;34m(self, x, index, ptr, dim_size, dim, reduce)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[39mreturn\u001b[39;00m segment(x, ptr, reduce\u001b[39m=\u001b[39mreduce)\n\u001b[1;32m    175\u001b[0m \u001b[39massert\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m \u001b[39mreturn\u001b[39;00m scatter(x, index, dim, dim_size, reduce)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 定义模型、算法、损失函数\n",
    "model = SAGEandLSTMModel(\n",
    "    input_size=input_size,\n",
    "    output_size=output_size,\n",
    ").to(device)\n",
    "# 考虑是否加入weight_decay\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# 学习率衰减\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.2)\n",
    "# criterion = nn.MSELoss(reduction=\"sum\")\n",
    "criterion = nn.MSELoss().to(device)\n",
    "\n",
    "model = train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    train_graph_list=train_graph_list,\n",
    "    val_graph_list=val_graph_list,\n",
    "    num_epochs=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把所有数据都放进模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 定义模型、算法、损失函数\n",
    "# model = GATv2Model(\n",
    "#     input_size=input_size,\n",
    "#     output_size=output_size,\n",
    "#     num_heads=num_heads,\n",
    "# ).to(device)\n",
    "# # 考虑是否加入weight_decay\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# model = train(model, criterion, optimizer, graphs, num_epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "model_path = \"saved/models/sage+lstm-s=1.pth\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取模型\n",
    "model = GATv2Model(\n",
    "    input_size=input_size,\n",
    "    output_size=output_size,\n",
    "    num_heads=num_heads,\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取node_ids\n",
    "with open(\"data/node_ids.pkl\", \"rb\") as file:\n",
    "    node_ids = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取scaler\n",
    "import pickle\n",
    "\n",
    "with open(\"saved/scaler/label_scaler.pkl\", \"rb\") as file:\n",
    "    label_scaler = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取测试集\n",
    "with open(\"data/test_graphs_data.pkl\", \"rb\") as file:\n",
    "    test_graphs = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_graphs), test_graphs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "output_list = []\n",
    "with torch.no_grad():\n",
    "    for i, graph in enumerate(test_graphs):\n",
    "        graph = graph.to(device)\n",
    "        output = model(graph)\n",
    "        output_list.append(output.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_list), output_list[0].shape, output_list[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 缩放\n",
    "output_list = [label_scaler.inverse_transform(output) for output in output_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看看是不是缩放完成啦\n",
    "output_list[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把输出转成需要的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# 结果写入csv，分隔符为\\t\n",
    "output_path = \"output/gatv2_out.csv\"\n",
    "\n",
    "date_id = [20230404, 20230405, 20230406, 20230407]\n",
    "with open(output_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")  # 设置分隔符为制表符\n",
    "    writer.writerow([\"geohash_id\", \"consumption_level\", \"activity_level\", \"date_id\"])\n",
    "\n",
    "    # output_list.shape = [4天, 节点数, 2个输出]\n",
    "    for nidx in range(len(node_ids)):\n",
    "        for day in range(len(date_id)):\n",
    "            # 注意不要写反了\n",
    "            writer.writerow(\n",
    "                [\n",
    "                    node_ids[nidx],\n",
    "                    output_list[day][nidx][1],\n",
    "                    output_list[day][nidx][0],\n",
    "                    date_id[day],\n",
    "                ]\n",
    "            )\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
