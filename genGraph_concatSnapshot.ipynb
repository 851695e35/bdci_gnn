{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.data import Data\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveData(data, path):\n",
    "    with open(path, \"wb\") as file:\n",
    "        pickle.dump(data, file)\n",
    "        file.close()\n",
    "\n",
    "\n",
    "def loadData(path):\n",
    "    data = None\n",
    "    with open(path, \"rb\") as file:\n",
    "        data = pickle.load(file)\n",
    "        file.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取dataframe并且处理一下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取node\n",
    "node_df_train = pd.read_csv(\"data/raw/train_90.csv\")\n",
    "# 删除全0列\n",
    "drop_c = [\"F_23\", \"F_27\"]\n",
    "node_df_train.drop(drop_c, axis=1, inplace=True)\n",
    "node_ids = node_df_train[\"geohash_id\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_ids.index(\"5324516fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化\n",
    "id_and_date_columns = [\"geohash_id\", \"date_id\"]\n",
    "label_columns = [\"active_index\", \"consume_index\"]\n",
    "feature_columns = node_df_train.drop(\n",
    "    id_and_date_columns + label_columns, axis=1\n",
    ").columns\n",
    "\n",
    "feature_scaler = StandardScaler()\n",
    "node_df_train.loc[:, feature_columns] = feature_scaler.fit_transform(\n",
    "    node_df_train[feature_columns]\n",
    ")\n",
    "label_scaler = StandardScaler()\n",
    "node_df_train.loc[:, label_columns] = label_scaler.fit_transform(\n",
    "    node_df_train[label_columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读边"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_df_train = pd.read_csv(\"data/raw/edge_90.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化\n",
    "edge_feature_columns = [\"F_1\", \"F_2\"]\n",
    "edge_feature_scaler = StandardScaler()\n",
    "edge_df_train.loc[:, edge_feature_columns] = edge_feature_scaler.fit_transform(\n",
    "    edge_df_train[edge_feature_columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建图\n",
    "\n",
    "每seq_len天的图连接起来，构成一个大图\n",
    "\n",
    "连接方法：第t天的图和第t+1天的图之间，相同的节点加一条边"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildGraph(node_df, edge_df, is_test_dataset=False):\n",
    "    graphs = []\n",
    "\n",
    "    # 按照date_id分组\n",
    "    for date_id, graph in node_df.groupby(\"date_id\"):\n",
    "        # 节点\n",
    "        x = torch.tensor(graph[feature_columns].values, dtype=torch.float)\n",
    "        if not is_test_dataset:\n",
    "            y = torch.tensor(graph[label_columns].values, dtype=torch.float)\n",
    "\n",
    "        # 边\n",
    "        edge_day_df = edge_df[edge_df[\"date_id\"] == date_id]\n",
    "        edge_index, edge_attr = [], []\n",
    "        for _, edge in edge_day_df.iterrows():\n",
    "            # 边可能给多了，只取存在的\n",
    "            if (\n",
    "                edge[\"geohash6_point1\"] not in node_ids\n",
    "                or edge[\"geohash6_point2\"] not in node_ids\n",
    "            ):\n",
    "                continue\n",
    "            edge_index.append(\n",
    "                [\n",
    "                    node_ids.index(edge[\"geohash6_point1\"]),\n",
    "                    node_ids.index(edge[\"geohash6_point2\"]),\n",
    "                ]\n",
    "            )\n",
    "            edge_attr.append(edge[edge_feature_columns].values.astype(np.float32))\n",
    "        edge_index = torch.tensor(np.array(edge_index), dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(np.array(edge_attr), dtype=torch.float)\n",
    "\n",
    "        # 图\n",
    "        if is_test_dataset:\n",
    "            graph = [x, edge_index, edge_attr]\n",
    "        else:\n",
    "            graph = [x, edge_index, edge_attr, y]\n",
    "        graphs.append(graph)\n",
    "\n",
    "        print(date_id, \"finished\")\n",
    "\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphs = buildGraph(node_df_train, edge_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把这里的路径改成你本地存放train_graphs_data.pkl的路径\n",
    "graphs = loadData(\"data/concat_graph/raw_graphs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 4, 4)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graphs), len(graphs[0]), len(graphs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建图第二步\n",
    "\n",
    "每seq_len天的图连接起来，构成一个大图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatGraph(graphs, seq_len=7, stride=3, is_test_dataset=False):\n",
    "    concat_graphs = []\n",
    "    for i in range(0, len(graphs) - seq_len + 1, stride):\n",
    "        print(i, i + seq_len)\n",
    "        raw_graphs = graphs[i : i + seq_len]\n",
    "        x = torch.cat([graph[0] for graph in raw_graphs], dim=0)\n",
    "        if not is_test_dataset:\n",
    "            y = torch.cat([graph[3] for graph in raw_graphs], dim=0)\n",
    "\n",
    "        # edge_attr 先不要了，因为sage用不到\n",
    "\n",
    "        # 拿到所有的边\n",
    "        edge_index = torch.cat(\n",
    "            [graph[1] + gnum * len(node_ids) for gnum, graph in enumerate(raw_graphs)],\n",
    "            dim=1,\n",
    "        )\n",
    "        # 生成新的边：每个节点和下一个时间片的节点相连\n",
    "        new_edge_index = (\n",
    "            torch.tensor(\n",
    "                np.array(\n",
    "                    [\n",
    "                        [n, n + len(node_ids)]\n",
    "                        for n in range(len(node_ids) * (seq_len - 1))\n",
    "                    ]\n",
    "                ),\n",
    "                dtype=torch.long,\n",
    "            )\n",
    "            .t()\n",
    "            .contiguous()\n",
    "        )\n",
    "        # 连接原有边和新边\n",
    "        edge_index = torch.cat([edge_index, new_edge_index], dim=1)\n",
    "\n",
    "        # print(x.shape, y.shape, edge_index.shape)\n",
    "        if is_test_dataset:\n",
    "            concat_graphs.append(Data(x=x, edge_index=edge_index))\n",
    "        else:\n",
    "            concat_graphs.append(Data(x=x, y=y, edge_index=edge_index))\n",
    "\n",
    "    return concat_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7\n",
      "1 8\n",
      "2 9\n",
      "3 10\n",
      "4 11\n",
      "5 12\n",
      "6 13\n",
      "7 14\n",
      "8 15\n",
      "9 16\n",
      "10 17\n",
      "11 18\n",
      "12 19\n",
      "13 20\n",
      "14 21\n",
      "15 22\n",
      "16 23\n",
      "17 24\n",
      "18 25\n",
      "19 26\n",
      "20 27\n",
      "21 28\n",
      "22 29\n",
      "23 30\n",
      "24 31\n",
      "25 32\n",
      "26 33\n",
      "27 34\n",
      "28 35\n",
      "29 36\n",
      "30 37\n",
      "31 38\n",
      "32 39\n",
      "33 40\n",
      "34 41\n",
      "35 42\n",
      "36 43\n",
      "37 44\n",
      "38 45\n",
      "39 46\n",
      "40 47\n",
      "41 48\n",
      "42 49\n",
      "43 50\n",
      "44 51\n",
      "45 52\n",
      "46 53\n",
      "47 54\n",
      "48 55\n",
      "49 56\n",
      "50 57\n",
      "51 58\n",
      "52 59\n",
      "53 60\n",
      "54 61\n",
      "55 62\n",
      "56 63\n",
      "57 64\n",
      "58 65\n",
      "59 66\n",
      "60 67\n",
      "61 68\n",
      "62 69\n",
      "63 70\n",
      "64 71\n",
      "65 72\n",
      "66 73\n",
      "67 74\n",
      "68 75\n",
      "69 76\n",
      "70 77\n",
      "71 78\n",
      "72 79\n",
      "73 80\n",
      "74 81\n",
      "75 82\n",
      "76 83\n",
      "77 84\n",
      "78 85\n",
      "79 86\n",
      "80 87\n",
      "81 88\n",
      "82 89\n",
      "83 90\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_graphs = concatGraph(graphs, seq_len=seq_len, stride=1)\n",
    "len(concat_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Data(x=[7980, 33], edge_index=[2, 85761], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 85360], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 84850], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 84854], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 85480], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 85425], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 85565], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 85727], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 86086], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 86513], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87617], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87942], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87962], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 88042], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 88100], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87772], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87398], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87211], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87094], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87049], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87058], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 86890], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87129], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87617], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87328], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 86863], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87095], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87025], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 86906], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 86943], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 86553], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 86796], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 86801], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 83952], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 81809], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 81607], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 81597], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 81985], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 82041], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 82448], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 85356], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87583], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 88104], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 88093], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87973], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87793], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87706], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87598], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87602], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87457], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87423], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87460], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87652], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87671], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87486], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87226], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87312], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87298], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87075], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87299], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87230], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87301], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87317], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87265], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 86994], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 86135], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 86003], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 86247], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 86534], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 86870], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87269], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 87963], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 89251], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 89329], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 88977], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 89303], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 89517], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 89391], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 89362], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 89384], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 89575], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 90209], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 90061], y=[7980, 2]),\n",
       " Data(x=[7980, 33], edge_index=[2, 90086], y=[7980, 2])]"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveData(concat_graphs, \"data/concat_graph/train_graphs_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存node_id和index的对应关系\n",
    "saveData(node_ids * seq_len, \"data/concat_graph/node_ids.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成训练集图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取node\n",
    "node_df_test = pd.read_csv(\"data/raw/A榜/node_test_4_A.csv\")\n",
    "node_df_test.drop(drop_c, axis=1, inplace=True)\n",
    "node_ids = loadData(\"data/concat_graph/node_ids.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试集中有一些不正确的node_id，需要进行转化\n",
    "special_node = {\n",
    "    \"18377236\": \"018377236\",\n",
    "    \"7.45E+07\": \"7449766e1\",\n",
    "    \"9.80E+10\": \"9797336e4\",\n",
    "}\n",
    "node_df_test.replace(special_node, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在测试集中的node_id都是正确的\n",
    "assert node_df_test[\"geohash_id\"].isin(node_ids).value_counts().item() == len(\n",
    "    node_df_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化\n",
    "node_df_test.loc[:, feature_columns] = feature_scaler.transform(\n",
    "    node_df_test[feature_columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "边"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_df_test = pd.read_csv(\"data/raw/A榜/edge_test_4_A.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化\n",
    "edge_df_test.loc[:, edge_feature_columns] = edge_feature_scaler.transform(\n",
    "    edge_df_test[edge_feature_columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_graphs = buildGraph(node_df_test, edge_df_test, is_test_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graphs = loadData(\"data/concat_graph/raw_test_graphs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveData(test_graphs, \"data/concat_graph/raw_test_graphs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7\n",
      "1 8\n",
      "2 9\n",
      "3 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_test_graphs = concatGraph(\n",
    "    graphs[-seq_len + 1 :] + test_graphs,\n",
    "    seq_len=seq_len,\n",
    "    stride=1,\n",
    "    is_test_dataset=True,\n",
    ")\n",
    "len(concat_test_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveData(concat_test_graphs, \"data/concat_graph/test_graphs_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存所有标准化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveData(feature_scaler, \"saved/scaler/feature_scaler.pkl\")\n",
    "# saveData(label_scaler, \"saved/scaler/label_scaler.pkl\")\n",
    "# saveData(edge_feature_scaler, \"saved/scaler/edge_feature_scaler.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
